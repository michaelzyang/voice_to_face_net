{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T20:06:28.858363Z",
     "start_time": "2019-11-17T20:06:27.468039Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:13:38.902595Z",
     "start_time": "2019-11-21T18:13:38.894932Z"
    }
   },
   "outputs": [],
   "source": [
    "# class VoiceMFCC(Dataset):\n",
    "#     \"\"\"Voice MFCC spectra dataset.\"\"\"\n",
    "\n",
    "#     def __init__(self, csv_files, standardize=False):\n",
    "#         \"\"\"\n",
    "#         Preconditions: csv files must contain matrices of the same dimension\n",
    "#         Args:\n",
    "#             csv_files (string or list): list of filenames/pathnames of csv files \n",
    "#                                         with MFCC spectrogram matrices\n",
    "#             transform (callable, optional): Optional transform to be applied\n",
    "#                                             on a sample.\n",
    "#         \"\"\"\n",
    "#         # ensure csv_files is a list\n",
    "#         if type(csv_files) == str:\n",
    "#             csv_files = [csv_files]\n",
    "        \n",
    "#         # load csv files with the MFCC spectra into a 3D tensor\n",
    "#         matrices = []\n",
    "#         for f in csv_files:\n",
    "#             matrix = np.loadtxt(f, delimiter=',', dtype=np.float32)\n",
    "#             if standardize:\n",
    "#                 matrix = (matrix - np.mean(matrix)) / np.std(matrix)\n",
    "#             matrices.append(matrix)\n",
    "#         self.X = torch.Tensor(matrices)\n",
    "#         N, D, M = self.X.shape\n",
    "#         self.X = self.X.view(N, 1, D, M) # insert channel dimension\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return self.X.shape[0]\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         sample = self.X[idx]\n",
    "\n",
    "#         return sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:13:38.902595Z",
     "start_time": "2019-11-21T18:13:38.894932Z"
    }
   },
   "outputs": [],
   "source": [
    "class voice_face(Dataset):\n",
    "    def __init__(self, csv_spectra, face_IDs, standardize=False):\n",
    "        \"\"\"\n",
    "        Preconditions: csv files must contain matrices of the same dimension\n",
    "        Args:\n",
    "            csv_spectra (string or list): list of filenames/pathnames of csv files with spectrogram matrices\n",
    "            face_ID (list or array):      list or array of corresponding face index numbers\n",
    "            standardise (boolean):        whether to standardize the spectrograms\n",
    "        \"\"\"\n",
    "        # ensure csv_spectra is a list\n",
    "        if type(csv_spectra) == str:\n",
    "            csv_spectra = [csv_spectra]\n",
    "        \n",
    "        # ensure same number of observations\n",
    "        assert(len(csv_spectra) = len(list(face_IDs)))\n",
    "        \n",
    "        # load csv files with the spectra into a tensor\n",
    "        matrices = []\n",
    "        for f in csv_spectra:\n",
    "            matrix = np.loadtxt(f, delimiter=',', dtype=np.float32)\n",
    "            if standardize:\n",
    "                matrix = (matrix - np.mean(matrix)) / np.std(matrix)\n",
    "            matrices.append(matrix)\n",
    "        self.X = torch.Tensor(matrices)\n",
    "        N, D, M = self.X.shape\n",
    "        self.X = self.X.view(N, 1, D, M) # insert channel dimension\n",
    "        \n",
    "        # save face_IDs\n",
    "        self.y = torch.tensor(face_IDs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:16:23.976995Z",
     "start_time": "2019-11-21T18:14:15.792467Z"
    }
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "csv_files = glob(\"data/train_mfcc/*.csv\")\n",
    "# dataset = VoiceMFCC(csv_files, standardize=True)\n",
    "dataset = VoiceMFCC(csv_files[:10], standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:16:23.990496Z",
     "start_time": "2019-11-21T18:16:23.981863Z"
    }
   },
   "outputs": [],
   "source": [
    "# split dataset\n",
    "TRAIN_SIZE = 9099\n",
    "VALID_SIZE = 2000\n",
    "TEST_SIZE = 2000\n",
    "train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(dataset, \n",
    "                                                                                [TRAIN_SIZE, \n",
    "                                                                                 VALID_SIZE, \n",
    "                                                                                 TEST_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:16:23.998261Z",
     "start_time": "2019-11-21T18:16:23.994329Z"
    }
   },
   "outputs": [],
   "source": [
    "# pipe data through a dataloader for batching\n",
    "BATCH_SIZE = 20\n",
    "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T21:58:03.542933Z",
     "start_time": "2019-11-21T21:58:03.527343Z"
    }
   },
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1), \n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d(2, stride=2), \n",
    "                nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "                nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "                nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.ModuleList(\n",
    "            [\n",
    "                nn.ConvTranspose2d(1, 64, kernel_size=3, stride=1, padding=1), \n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(64, 16, kernel_size=3, stride=2, padding=(1,0),output_padding = (1,0)), \n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=(1,1),output_padding = (1,1)),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(8, 1, kernel_size=3, stride=2, padding=(1,0), output_padding = (1,0)),\n",
    "                nn.Tanh()\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.w_length = 5\n",
    "        self.face_length = 128*128\n",
    "        self.B = nn.Linear(self.w_length, self.face_length, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(\"shape start: {}\".format(x.shape))\n",
    "        \n",
    "#         print(\"start encoder\")\n",
    "        for layer in self.encoder:\n",
    "            x = layer.forward(x)\n",
    "            print(x.shape)\n",
    "        \n",
    "        # collapse final feature map into a vector by taking average across time\n",
    "        N, _, H, _ = x.shape\n",
    "        w = x.mean(dim=3)\n",
    "        w = w.view(N, H)\n",
    "        \n",
    "#         print(\"start decoder\")     \n",
    "        for layer in self.decoder:\n",
    "            v = layer.forward(x)\n",
    "            print(v.shape)\n",
    "            \n",
    "#         print(\"start face construction\")\n",
    "        f = self.B(w)\n",
    "        \n",
    "        return v, f, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1\n",
    "def my_loss(model_output,labels):\n",
    "    voice_output, face_output = model_output\n",
    "    \n",
    "    voice_data, face_id = labels\n",
    "    face_data = face_struct[face_id]\n",
    "    \n",
    "    loss = fid(face_output, face_data) + ALPHA * MSE(voice_output, voice_data)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T21:58:14.762519Z",
     "start_time": "2019-11-21T21:58:14.755729Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = autoencoder().cuda()\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "model = autoencoder()\n",
    "MSE = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:54:09.705752Z",
     "start_time": "2019-11-21T18:54:09.701928Z"
    }
   },
   "outputs": [],
   "source": [
    "# for data in dataloader:\n",
    "#     temp = data\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:54:10.059305Z",
     "start_time": "2019-11-21T18:54:10.055300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 40, 173])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch in dataloader:\n",
    "        # ===================forward=====================\n",
    "        voice, face_id = batch\n",
    "        voice_output, face_output = model(voice)\n",
    "        loss = my_loss((voice_output,face_output), (voice,face_id))\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    if((epoch+1)%10 == 0):\n",
    "        print('epoch [{}/{}], loss:{:.4f}'\n",
    "              .format(epoch+1, NUM_EPOCHS, loss.data.item()))\n",
    "\n",
    "torch.save(model.state_dict(), './model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that NN outputs the same size file as input\n",
    "# FID for similarity between faces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
