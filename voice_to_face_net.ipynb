{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T20:06:28.858363Z",
     "start_time": "2019-11-17T20:06:27.468039Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T21:50:43.496466Z",
     "start_time": "2019-11-24T21:50:43.448774Z"
    }
   },
   "outputs": [],
   "source": [
    "class voice_face(Dataset):\n",
    "    def __init__(self, voice_filenames, standardize=False):\n",
    "        \"\"\"\n",
    "        Preconditions: csv files must contain matrices of the same dimension\n",
    "        Args:\n",
    "            voice_filenames (string or list): list of filenames/pathnames of csv files with spectrogram matrices\n",
    "                                              assumes format voice_{n}_{m}.csv, \n",
    "                                              where n is the data ID and m is the spectrogram number for that speaker\n",
    "            standardise (boolean):            whether to standardize the spectrograms\n",
    "        \"\"\"\n",
    "        # ensure inputs are lists\n",
    "        if type(voice_filenames) == str:\n",
    "            voice_filenames = [voice_filenames]\n",
    "        assert(type(voice_filenames) == list)\n",
    "                \n",
    "        # load voice spectrograms one by one\n",
    "        face_IDs = [] # the face IDs associated with each spectrogram\n",
    "        matrices = [] # the spectrograms\n",
    "        for v_file in voice_filenames:\n",
    "            # get n, the data ID \n",
    "            n, _ = get_n_m(v_file)\n",
    "            face_IDs.append(n)\n",
    "            \n",
    "            # get spectrogram\n",
    "            matrix = np.loadtxt(v_file, delimiter=',', dtype=np.float32)\n",
    "            if standardize:\n",
    "                matrix = (matrix - np.mean(matrix)) / np.std(matrix)\n",
    "            matrices.append(matrix)\n",
    "        \n",
    "        # construct spectrograms tensor\n",
    "        self.X = torch.Tensor(matrices)\n",
    "        N, D, M = self.X.shape\n",
    "        self.X = self.X.view(N, 1, D, M) # insert channel dimension\n",
    "        \n",
    "        # construct face_IDs tensor\n",
    "        self.y = torch.tensor(face_IDs)\n",
    "        \n",
    "        assert(self.X.shape[0] == self.y.shape[0])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T21:50:45.798706Z",
     "start_time": "2019-11-24T21:50:45.794797Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_n_m(v_file):\n",
    "    v_file = v_file.split('/')[-1] # strip the pathname if it exists\n",
    "    v_file, _ = v_file.split('.') # strip the file extension\n",
    "    _, n, m = v_file.split('_') # get n and m from the filename\n",
    "    return n, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:16:23.976995Z",
     "start_time": "2019-11-21T18:14:15.792467Z"
    }
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "voice_filenames = glob(\"data/voice_*.csv\")\n",
    "face_filenames = glob(\"data/face_*.csv\")\n",
    "dataset = VoiceMFCC(voice_filenames, standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:16:23.990496Z",
     "start_time": "2019-11-21T18:16:23.981863Z"
    }
   },
   "outputs": [],
   "source": [
    "# split dataset\n",
    "TRAIN_SIZE = 9099\n",
    "VALID_SIZE = 2000\n",
    "TEST_SIZE = 2000\n",
    "train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(dataset, \n",
    "                                                                                [TRAIN_SIZE, \n",
    "                                                                                 VALID_SIZE, \n",
    "                                                                                 TEST_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:16:23.998261Z",
     "start_time": "2019-11-21T18:16:23.994329Z"
    }
   },
   "outputs": [],
   "source": [
    "# pipe data through a dataloader for batching\n",
    "BATCH_SIZE = 20\n",
    "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T21:58:03.542933Z",
     "start_time": "2019-11-21T21:58:03.527343Z"
    }
   },
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self, w_length, face_length):\n",
    "        \"\"\"\n",
    "        w_length: the length of the bottleneck vector i.e. # of basis faces used\n",
    "        face_length: the height * width of the face images\n",
    "        \"\"\"\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1), \n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d(2, stride=2), \n",
    "                nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "                nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "                nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.ModuleList(\n",
    "            [\n",
    "                nn.ConvTranspose2d(1, 64, kernel_size=3, stride=1, padding=1), \n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(64, 16, kernel_size=3, stride=2, padding=(1,0),output_padding = (1,0)), \n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=(1,1),output_padding = (1,1)),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(8, 1, kernel_size=3, stride=2, padding=(1,0), output_padding = (1,0)),\n",
    "                nn.Tanh()\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.w_length = w_length\n",
    "        self.face_length = face_length\n",
    "        self.B = nn.Linear(self.w_length, self.face_length, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # start encoder\n",
    "        for layer in self.encoder:\n",
    "            x = layer.forward(x)\n",
    "#             print(x.shape)\n",
    "        \n",
    "        # collapse final feature map into a vector by taking average across time\n",
    "        N, _, H, _ = x.shape\n",
    "        w = x.mean(dim=3)\n",
    "        w = w.view(N, H)\n",
    "        \n",
    "#         # start decoder\n",
    "#         for layer in self.decoder:\n",
    "#             v = layer.forward(x)\n",
    "# #             print(v.shape)\n",
    "            \n",
    "        # face construction\n",
    "        f = self.B(w)\n",
    "        \n",
    "#         return v, f, w\n",
    "        return f, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1\n",
    "def combined_loss(model_output,labels):\n",
    "    voice_output, face_output = model_output\n",
    "    \n",
    "    voice_data, face_id = labels\n",
    "    face_data = face_struct[face_id]\n",
    "    \n",
    "    loss = fid(face_output, face_data) + ALPHA * MSE(voice_output, voice_data)\n",
    "    return loss\n",
    "\n",
    "def face_loss(face_outputs, labels):\n",
    "    true_faces = face_struct[labels]\n",
    "#     loss = fid(face_output, true_faces)\n",
    "    loss = torch.dist(face_output, true_faces, p=2) ** 2 # squared L2 norm\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T21:58:14.762519Z",
     "start_time": "2019-11-21T21:58:14.755729Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = autoencoder().cuda()\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "model = autoencoder()\n",
    "MSE = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:54:09.705752Z",
     "start_time": "2019-11-21T18:54:09.701928Z"
    }
   },
   "outputs": [],
   "source": [
    "# for data in dataloader:\n",
    "#     temp = data\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:54:10.059305Z",
     "start_time": "2019-11-21T18:54:10.055300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 40, 173])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch in dataloader:\n",
    "        # ===================forward=====================\n",
    "        voice, face_id = batch\n",
    "        voice_output, face_output, w = model(voice)\n",
    "        loss = my_loss((voice_output,face_output), (voice,face_id))\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    if((epoch+1)%10 == 0):\n",
    "        print('epoch [{}/{}], loss:{:.4f}'\n",
    "              .format(epoch+1, NUM_EPOCHS, loss.data.item()))\n",
    "\n",
    "torch.save(model.state_dict(), './model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that NN outputs the same size file as input\n",
    "# FID for similarity between faces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
