{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T20:06:28.858363Z",
     "start_time": "2019-11-17T20:06:27.468039Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:13:38.902595Z",
     "start_time": "2019-11-21T18:13:38.894932Z"
    }
   },
   "outputs": [],
   "source": [
    "class VoiceMFCC(Dataset):\n",
    "    \"\"\"Voice MFCC spectra dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_files, standardize=False):\n",
    "        \"\"\"\n",
    "        Preconditions: csv files must contain matrices of the same dimension\n",
    "        Args:\n",
    "            csv_files (string or list): list of filenames/pathnames of csv files \n",
    "                                        with MFCC spectrogram matrices\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                                            on a sample.\n",
    "        \"\"\"\n",
    "        # ensure csv_files is a list\n",
    "        if type(csv_files) == str:\n",
    "            csv_files = [csv_files]\n",
    "        \n",
    "        # load csv files with the MFCC spectra into a 3D tensor\n",
    "        matrices = []\n",
    "        for f in csv_files:\n",
    "            matrix = np.loadtxt(f, delimiter=',', dtype=np.float32)\n",
    "            if standardize:\n",
    "                matrix = (matrix - np.mean(matrix)) / np.std(matrix)\n",
    "            matrices.append(matrix)\n",
    "        self.X = torch.Tensor(matrices)\n",
    "        N, D, M = self.X.shape\n",
    "        self.X = self.X.view(N, 1, D, M) # THIS LINE DIDNT SEEM TO FIX OUR PROBLEM...\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx]\n",
    "\n",
    "        return sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:14:13.299683Z",
     "start_time": "2019-11-21T18:14:13.296954Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_3D = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:16:23.976995Z",
     "start_time": "2019-11-21T18:14:15.792467Z"
    }
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "csv_files = glob(\"data/train_mfcc/*.csv\")\n",
    "dataset = VoiceMFCC(csv_files, standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:16:23.990496Z",
     "start_time": "2019-11-21T18:16:23.981863Z"
    }
   },
   "outputs": [],
   "source": [
    "# split dataset\n",
    "# TRAIN_SIZE = 9099\n",
    "# VALID_SIZE = 2000\n",
    "# TEST_SIZE = 2000\n",
    "TRAIN_SIZE = 100\n",
    "VALID_SIZE = 10999\n",
    "TEST_SIZE = 2000\n",
    "train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(dataset, \n",
    "                                                                                [TRAIN_SIZE, \n",
    "                                                                                 VALID_SIZE, \n",
    "                                                                                 TEST_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:16:23.998261Z",
     "start_time": "2019-11-21T18:16:23.994329Z"
    }
   },
   "outputs": [],
   "source": [
    "# pipe data through a dataloader for batching\n",
    "BATCH_SIZE = 20\n",
    "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T21:05:47.914373Z",
     "start_time": "2019-11-17T21:05:47.904252Z"
    }
   },
   "outputs": [],
   "source": [
    "# class autoencoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(autoencoder, self).__init__()\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Conv2d(1, 8, 3, stride=3, padding=1), \n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(2, stride=2), \n",
    "#             nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(2, stride=2) ,\n",
    "#             nn.Conv2d(16, 64, 3, stride=2, padding=1),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(2, stride=2),\n",
    "#             nn.Conv2d(64, 128, 3, stride=2, padding=1) \n",
    "#             #nn.ReLU(True)\n",
    "#             #nn.MaxPool2d(2, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(128, 64, 3, stride=2), \n",
    "#             nn.ReLU(True),\n",
    "#             nn.ConvTranspose2d(64, 16, 3, stride=2, padding=1), \n",
    "#             nn.ReLU(True),\n",
    "#             nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.ConvTranspose2d(8, 1, 3, stride=3, padding=1),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for i in self.decoder:\n",
    "#             i(x)\n",
    "#         x = self.encoder(x)\n",
    "#         x = self.decoder(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:54:09.076784Z",
     "start_time": "2019-11-21T18:54:09.064301Z"
    }
   },
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1), \n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d(2, stride=2), \n",
    "                nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "                nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "                nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.ModuleList(\n",
    "            [\n",
    "                nn.ConvTranspose2d(1, 64, kernel_size=3, stride=1, padding=1), \n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(64, 16, kernel_size=5, stride=2, padding=1), \n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(8, 1, kernel_size=3, stride=2, padding=1),\n",
    "                nn.Tanh()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"shape start: {}\".format(x.shape))\n",
    "        \n",
    "        print(\"start encoder\")\n",
    "        for layer in self.encoder:\n",
    "            x = layer.forward(x)\n",
    "            print(x.shape)\n",
    "        \n",
    "        # flatten bottleneck into vectors\n",
    "        N, _, H, W = x.shape\n",
    "        w = x.view(N, H * W)\n",
    "        \n",
    "        print(\"start decoder\")     \n",
    "        for layer in self.decoder:\n",
    "            x = layer.forward(x)\n",
    "            print(x.shape)\n",
    "        \n",
    "        return x, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:54:09.486102Z",
     "start_time": "2019-11-21T18:54:09.478532Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = autoencoder().cuda()\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "model = autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:54:09.705752Z",
     "start_time": "2019-11-21T18:54:09.701928Z"
    }
   },
   "outputs": [],
   "source": [
    "for data in dataloader:\n",
    "    temp = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:54:10.059305Z",
     "start_time": "2019-11-21T18:54:10.055300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 40, 173])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:54:10.772893Z",
     "start_time": "2019-11-21T18:54:10.711855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape start: torch.Size([20, 1, 40, 173])\n",
      "start encoder\n",
      "torch.Size([20, 8, 40, 173])\n",
      "torch.Size([20, 8, 40, 173])\n",
      "torch.Size([20, 8, 20, 86])\n",
      "torch.Size([20, 16, 20, 86])\n",
      "torch.Size([20, 16, 20, 86])\n",
      "torch.Size([20, 16, 10, 43])\n",
      "torch.Size([20, 64, 10, 43])\n",
      "torch.Size([20, 64, 10, 43])\n",
      "torch.Size([20, 64, 5, 21])\n",
      "torch.Size([20, 1, 5, 21])\n",
      "start decoder\n",
      "torch.Size([20, 64, 5, 21])\n",
      "torch.Size([20, 64, 5, 21])\n",
      "torch.Size([20, 16, 11, 43])\n",
      "torch.Size([20, 16, 11, 43])\n",
      "torch.Size([20, 8, 21, 85])\n",
      "torch.Size([20, 8, 21, 85])\n",
      "torch.Size([20, 1, 41, 169])\n",
      "torch.Size([20, 1, 41, 169])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-1249f117caa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# ===================forward=====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# ===================backward====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2189\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2190\u001b[0m     \"\"\"\n\u001b[0;32m-> 2191\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2192\u001b[0m         warnings.warn(\"Using a target size ({}) that is different to the input size ({}). \"\n\u001b[1;32m   2193\u001b[0m                       \u001b[0;34m\"This will likely lead to incorrect results due to broadcasting. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch in dataloader:\n",
    "        # ===================forward=====================\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch+1, num_epochs, loss.data.item())) # EDIT: changed [0] to .item()\n",
    "\n",
    "torch.save(model.state_dict(), './model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that NN outputs the same size file as input\n",
    "# FID for similarity between faces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
