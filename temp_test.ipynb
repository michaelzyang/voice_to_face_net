{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T03:58:05.632848Z",
     "start_time": "2019-11-30T03:58:05.613946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing voice data. 2019-11-29 22:58:05.615376\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing voice data. {}\".format(datetime.now()))\n",
    "train_voice_filenames = get_filenames(voice_train_path)\n",
    "train_voice_filenames_small = train_voice_filenames[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T03:58:11.914608Z",
     "start_time": "2019-11-30T03:58:05.941583Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset_small = voice_face(train_voice_filenames_small, standardize=True)\n",
    "dataloader_small = DataLoader(train_dataset_small, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T03:58:12.171204Z",
     "start_time": "2019-11-30T03:58:11.917389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing face data as vectors into a dictionary. 2019-11-29 22:58:11.920195\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing face data as vectors into a dictionary. {}\".format(datetime.now()))\n",
    "train_IDs = set(train_dataset_small.y.unique()) # type tensor\n",
    "# validate_IDs = set(validate_dataset.y.unique())\n",
    "# IDs = train_IDs.union(validate_IDs)\n",
    "# face_dict = make_face_dict(IDs, path=face_file_format)\n",
    "face_dict = make_face_dict(train_IDs, path=face_file_format, face_std=FACE_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T03:21:18.049945Z",
     "start_time": "2019-11-30T03:21:15.268016Z"
    }
   },
   "outputs": [],
   "source": [
    "# train AE_model and save outputs\n",
    "print(\"Loading Voice Autoencoder model. {}\".format(datetime.now()))\n",
    "AE_model = Voice_Autoencoder()\n",
    "AE_optimizer = torch.optim.Adam(AE_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "AE_NUM_EPOCHS = 1    \n",
    "\n",
    "AE_loss_epochs = []\n",
    "for epoch in range(AE_NUM_EPOCHS):\n",
    "    for batch in dataloader_small:\n",
    "        # ===================forward=====================\n",
    "        voice_data, IDs = batch\n",
    "        voice_outputs, w = AE_model(voice_data)\n",
    "        loss = voice_loss(voice_outputs, voice_data)\n",
    "        # ===================backward====================\n",
    "        AE_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        AE_optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}, completed at {}'\n",
    "        .format(epoch+1, AE_N, loss.data.item(), datetime.now()))\n",
    "    AE_loss_epochs.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T03:14:32.133011Z",
     "start_time": "2019-11-30T03:14:32.126753Z"
    }
   },
   "outputs": [],
   "source": [
    "save_state(\"./AE_model_state_test.pth\", AE_model, AE_optimizer, [999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T03:14:40.008646Z",
     "start_time": "2019-11-30T03:14:39.998735Z"
    }
   },
   "outputs": [],
   "source": [
    "load_state(\"./AE_model_state_test.pth\", AE_model, AE_optimizer, print_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T03:49:55.832923Z",
     "start_time": "2019-11-30T03:45:24.793097Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Training model. {}\".format(datetime.now()))\n",
    "model = full_model(AE_model, face_shape=(128,128))\n",
    "if CUDA:\n",
    "    model = model.cuda()\n",
    "train_model(model, dataloader_small, face_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T03:56:59.744823Z",
     "start_time": "2019-11-30T03:56:59.680756Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script for training a given model architecture\n",
    "v0.2 November 29, 2019\n",
    "\n",
    "Script to train a voice to voice+face model for given hyperparameters and a \n",
    "given (trained) voice autoencoder.\n",
    "\n",
    "INPUTS:\n",
    "- a trained voice autoencoder - edit AE_save_state to be the filepath of its save state \n",
    "- edit the hyperparameters listed before the main function definition\n",
    "- note: provide the training voice data ONLY, not the voices for any IDs in the \n",
    "        validation or test sets, as we do not want the model to see any \n",
    "        validation or test set faces.\n",
    "OUTPUTS:\n",
    "- saves the full model model state at \"./model_state.pth\"\n",
    "- saves the convergence plot and list of loss at each iteration in a folder named \"./convergence\"\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# HS TODO tonight\n",
    "# give MY and JZ instructions for getting access to GCP and requesting high performance GPUs\n",
    "# upload full data to drive\n",
    "# start training voice AE tonight\n",
    "\n",
    "# MY TODO tonight\n",
    "# sign up for GCP and request approval for high performance GPUs\n",
    "# think of solution for face MSE being huge because its std is not 1.\n",
    "\n",
    "# JZ TODO tonight or asap tomorrow\n",
    "# - sign up for GCP and request approval for high performance GPUs\n",
    "# - ensure evaluation task can take a list of (reconstructed) face matrices, \n",
    "#       and list of IDs, and output an evaluation metric / list of evaluation metrics\n",
    "#       you can assume that a face_dict will be provided where face_dict[ID] returns\n",
    "#       the true face matrix associated with the ID.\n",
    "# - think about what chart(s) we can show on the results section of the poster for the evaluation.\n",
    "\n",
    "# TODO this weekend\n",
    "# - manually separate train, validate, test IDs and put them in separate folders\n",
    "# - write a predict function that averages the reconstructed face for all voice clips belonging to ID\n",
    "# - load each model state and get eval_metric on the evaluation task\n",
    "# - pick the model/hyperparams that yields best eval_metric\n",
    "# - optional: train model using best hyperparams on train+validate data\n",
    "# - get eval_metric of the best model using the test data\n",
    "\n",
    "\n",
    "\n",
    "ALPHA = 0\n",
    "ORTHOGONALIZE_B = False\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 10\n",
    "voice_loss = nn.MSELoss()\n",
    "face_loss = nn.MSELoss()\n",
    "FACE_STD = 28 # std dev of pixel values from a subsample of 93 faces. used to scale faces to have std ~= 1\n",
    "LEARNING_RATE = 1e-3\n",
    "CUDA = False\n",
    "voice_train_path = \"data/Voice_to_face/voicespecs/\"\n",
    "face_file_format = \"data/Voice_to_face/facespecs/face_{}.csv\"\n",
    "AE_save_state = \"./AE_model_state_test.pth\"\n",
    "\n",
    "\n",
    "# # EDIT THIS CLASS\n",
    "# class Voice_Autoencoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         \"\"\"\n",
    "#         w_length: the length of the bottleneck vector i.e. # of basis faces used\n",
    "#         face_length: the height * width of the face images\n",
    "#         \"\"\"\n",
    "#         super(Voice_Autoencoder, self).__init__()\n",
    "        \n",
    "#         self.w_length = None\n",
    "        \n",
    "#         self.encoder = nn.ModuleList(\n",
    "#             [\n",
    "#                 nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1), #1025 x 251\n",
    "#                 nn.ReLU(True),\n",
    "#                 #nn.MaxPool2d(2, stride=2), \n",
    "#                 nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=0), #512 x 125\n",
    "#                 nn.ReLU(True),\n",
    "#                 #nn.MaxPool2d(2, stride=2),\n",
    "#                 nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), #256 x 63\n",
    "#                 nn.ReLU(True),\n",
    "#                 nn.MaxPool2d(2, stride=2),                             #128 x 31\n",
    "#                 nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=1)   #128 x 31\n",
    "#             ]\n",
    "#         )\n",
    "        \n",
    "#         self.decoder = nn.ModuleList(\n",
    "#             [\n",
    "#                 nn.ConvTranspose2d(1, 32, kernel_size=3, stride=1, padding=1), # 128 x 31\n",
    "#                 nn.ReLU(True),\n",
    "#                 nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=(1,0), output_padding = (1,0)), # 256 x 63\n",
    "#                 nn.ReLU(True),\n",
    "#                 nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=(1,1), output_padding = (1,0)), # 512 x 125\n",
    "#                 nn.ReLU(True),\n",
    "#                 nn.ConvTranspose2d(8, 1, kernel_size=3, stride=2, padding=0, output_padding = 0), # 1025 x 251\n",
    "#                 nn.Tanh()\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "#     def forward(self, v):\n",
    "#         # start encoder\n",
    "#         for layer in self.encoder:\n",
    "#             v = layer.forward(v)\n",
    "#             #print(v.shape)\n",
    "        \n",
    "#         # collapse final feature map into a vector by taking average across time\n",
    "#         N, _, H, _ = v.shape\n",
    "#         w = v.mean(dim=3)\n",
    "#         w = w.view(N, H)\n",
    "        \n",
    "#         if self.w_length == None:\n",
    "#             self.w_length = H\n",
    "        \n",
    "#         # start decoder\n",
    "#         for layer in self.decoder:\n",
    "#             v = layer.forward(v)\n",
    "#             #print(v.shape)\n",
    "        \n",
    "#         return v, w\n",
    "\n",
    "\n",
    "# # EXAMPLE CODE FOR TRAINING VOICE AUTOENCODER\n",
    "# print(\"Importing voice data. {}\".format(datetime.now()))\n",
    "# train_dataset, dataloader = prep_data()\n",
    "# print(\"Importing face data as vectors into a dictionary. {}\".format(datetime.now()))\n",
    "# train_IDs = set(train_dataset.y.unique()) # type tensor\n",
    "# face_dict = make_face_dict(train_IDs, path=face_file_format, face_std=FACE_STD)\n",
    "\n",
    "# AE_model = Voice_Autoencoder()\n",
    "# AE_optimizer = torch.optim.Adam(AE_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "# AE_NUM_EPOCHS = 100\n",
    "\n",
    "# AE_loss_epochs = []\n",
    "# for epoch in range(AE_NUM_EPOCHS):\n",
    "#     for batch in dataloader:\n",
    "#         # ===================forward=====================\n",
    "#         voice_data, IDs = batch\n",
    "#         voice_outputs, w = AE_model(voice_data)\n",
    "#         loss = voice_loss(voice_outputs, voice_data)\n",
    "#         # ===================backward====================\n",
    "#         AE_optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         AE_optimizer.step()\n",
    "#     # ===================log========================\n",
    "#     print('epoch [{}/{}], loss:{:.4f}, completed at {}'\n",
    "#         .format(epoch+1, AE_NUM_EPOCHS, loss.data.item(), datetime.now()))\n",
    "#     AE_loss_epochs.append(loss)\n",
    "# save_state(\"./AE_model_state_test.pth\", AE_model, AE_optimizer, AE_loss_epochs)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Importing voice data. {}\".format(datetime.now()))\n",
    "    train_dataset, dataloader = prep_data()\n",
    "\n",
    "    print(\"Importing face data as vectors into a dictionary. {}\".format(datetime.now()))\n",
    "    train_IDs = set(train_dataset.y.unique()) # type tensor\n",
    "    face_dict = make_face_dict(train_IDs, path=face_file_format, face_std=FACE_STD)\n",
    "    \n",
    "    # train model and save outputs\n",
    "    print(\"Loading Voice Autoencoder model. {}\".format(datetime.now()))\n",
    "    AE_model = Voice_Autoencoder()\n",
    "    AE_optimizer = torch.optim.Adam(AE_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "    load_state(AE_save_state, AE_model, AE_optimizer, print_model=True)\n",
    "    # AE_model = TODO LOAD MODEL\n",
    "\n",
    "    print(\"Training model. {}\".format(datetime.now()))\n",
    "    model = full_model(AE_model, face_shape=(128,128))\n",
    "    if CUDA:\n",
    "        model = model.cuda()\n",
    "    train_model(model, dataloader, face_dict)\n",
    "\n",
    "    print(\"Model training complete. {}\".format(datetime.now()))\n",
    "\n",
    "\n",
    "class full_model(nn.Module):\n",
    "    def __init__(self, AE_model, face_shape=(128,128)):\n",
    "        \"\"\"\n",
    "        AE_model: a voice autoencoder model whose forward function returns v, w,\n",
    "                  where v is the voice reconstruction and w is the embedding\n",
    "        face_length: the height * width of the face images\n",
    "        \"\"\"\n",
    "        super(full_model, self).__init__()\n",
    "        self.AE_model = AE_model\n",
    "        self.w_length = AE_model.w_length\n",
    "        self.face_length = face_shape[0] * face_shape[1]\n",
    "        self.B = nn.Linear(self.w_length, self.face_length, bias=False)\n",
    "\n",
    "    def forward(self, v):\n",
    "        # run voice input through AE\n",
    "        v, w = self.AE_model(v)\n",
    "        \n",
    "        # face construction\n",
    "        f = self.B(w)\n",
    "        \n",
    "        return v, f\n",
    "\n",
    "\n",
    "def prep_data():\n",
    "    train_voice_filenames = get_filenames(voice_train_path)\n",
    "    train_dataset = voice_face(train_voice_filenames, standardize=True)\n",
    "    dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    return train_dataset, dataloader\n",
    "\n",
    "\n",
    "def make_face_dict(IDs, path=face_file_format, face_std=1):\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "    - IDs: a 1D tensor or iterable of int IDs\n",
    "    - path: a filename format with {} in place of the ID in the filename.\n",
    "    \"\"\"\n",
    "    # usage: face_dict[6] returns a 2D numpy array of the face with ID=6\n",
    "    face_dict = {}\n",
    "    for ID in IDs:\n",
    "        if type(ID) == torch.Tensor:\n",
    "            ID = ID.item()\n",
    "        assert(type(ID) == int)\n",
    "        \n",
    "        face_filename = face_file_format.format(ID)\n",
    "        face_arr = np.loadtxt(face_filename, delimiter=',').flatten()\n",
    "        face_arr = face_arr / face_std # scale to have std dev ~= 1\n",
    "        face_dict[ID] = face_arr\n",
    "    return face_dict\n",
    "\n",
    "\n",
    "def get_filenames(paths, filename_format=\"voice_*\"):\n",
    "    # in case only one path given, make it a list so that it's iterable\n",
    "    if type(paths) == str:\n",
    "        paths = [paths]\n",
    "\n",
    "    # get lists of all voice filenames\n",
    "    filenames = []\n",
    "    for path in paths:\n",
    "        if path[-1] != '/':\n",
    "            path += '/'\n",
    "        filenames += glob(path + filename_format)\n",
    "   \n",
    "    return filenames\n",
    "\n",
    "\n",
    "class voice_face(Dataset):\n",
    "    def __init__(self, voice_filenames, standardize=False):\n",
    "        \"\"\"\n",
    "        Preconditions: csv files must contain matrices of the same dimension\n",
    "        Args:\n",
    "            voice_filenames (string or list): list of filenames/pathnames of csv files with spectrogram matrices\n",
    "                                              assumes format voice_{n}_{m}.csv, \n",
    "                                              where n is the data ID and m is the spectrogram number for that speaker\n",
    "            standardise (boolean):            whether to standardize the spectrograms\n",
    "        \"\"\"\n",
    "        # ensure inputs are lists\n",
    "        if type(voice_filenames) == str:\n",
    "            voice_filenames = [voice_filenames]\n",
    "        assert(type(voice_filenames) == list)\n",
    "                \n",
    "        # load voice spectrograms one by one\n",
    "        face_IDs = [] # the face IDs associated with each spectrogram\n",
    "        matrices = [] # the spectrograms\n",
    "        for v_file in voice_filenames:\n",
    "            # get n, the data ID \n",
    "            n, _ = get_n_m(v_file)\n",
    "            face_IDs.append(n)\n",
    "            \n",
    "            # get spectrogram\n",
    "            matrix = np.loadtxt(v_file, delimiter=',', dtype=np.float32)\n",
    "            if standardize:\n",
    "                matrix = (matrix - np.mean(matrix)) / np.std(matrix)\n",
    "            matrices.append(matrix)\n",
    "        \n",
    "        # construct spectrograms tensor\n",
    "        self.X = torch.Tensor(matrices)\n",
    "        N, D, M = self.X.shape\n",
    "        self.X = self.X.view(N, 1, D, M) # insert channel dimension\n",
    "        \n",
    "        # construct face_IDs tensor\n",
    "        self.y = torch.tensor(face_IDs)\n",
    "        \n",
    "        assert(self.X.shape[0] == self.y.shape[0])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def get_n_m(v_file):\n",
    "    \"\"\"\n",
    "    Takes a voice file name of the form path/voice_{n}_{m}.csv\n",
    "    And outputs n and m as integers.\n",
    "    \"\"\"\n",
    "    v_file = v_file.split('/')[-1] # strip the pathname if it exists\n",
    "    v_file, _ = v_file.split('.') # strip the file extension\n",
    "    _, n, m = v_file.split('_') # get n and m from the filename\n",
    "    return int(n), int(m)\n",
    "\n",
    "def combined_loss(model_output, labels, face_dict):\n",
    "    \"\"\"\n",
    "    REQUIRES\n",
    "    - face dictionary\n",
    "    - face_retrieve_loss function\n",
    "    INPUTS\n",
    "    - model_output: a tuple of (voice_output, face_output), both tensors\n",
    "    - labels: a tuple of (voice_data, face_id), a tensor and a 1-D tensor\n",
    "    \"\"\"\n",
    "    # unpack input\n",
    "    voice_outputs, face_outputs = model_output\n",
    "    voice_data, IDs = labels\n",
    "    \n",
    "    # compute combined loss\n",
    "    combined_loss = face_retrieve_loss(face_outputs, IDs, face_dict) + ALPHA * voice_loss(voice_outputs, voice_data)\n",
    "    return combined_loss\n",
    "\n",
    "\n",
    "def face_retrieve_loss(face_outputs, IDs, face_dict):\n",
    "    \"\"\"\n",
    "    Retrieves ground truth face tensor given the IDs, computes and returns loss\n",
    "    REQUIRES\n",
    "    - face dictionary\n",
    "    - face_loss function\n",
    "    INPUTS\n",
    "    - face_outputs: tensor of face matrices\n",
    "    - IDs: tensor of IDs\n",
    "    \"\"\"\n",
    "    # construct true faces tensor\n",
    "    true_faces = []\n",
    "    for ID in IDs:\n",
    "        ID_int = ID.item()\n",
    "        true_faces.append(face_dict[ID_int])\n",
    "    true_faces = torch.Tensor(true_faces)\n",
    "\n",
    "    # compute and return loss\n",
    "    loss = face_loss(face_outputs, true_faces)\n",
    "    return loss\n",
    "\n",
    "def train_model(model, dataloader, face_dict):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "    \n",
    "    loss_epochs = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for batch in dataloader:\n",
    "            # ===================forward=====================\n",
    "            voice_data, IDs = batch\n",
    "            # voice_outputs, face_outputs = model(voice_data)\n",
    "            loss = combined_loss(model(voice_data), batch, face_dict)\n",
    "            # ===================backward====================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if ORTHOGONALIZE_B:\n",
    "                model.B.weight.data = gram_schmidt(model.B.weight.data)\n",
    "        # ===================log========================\n",
    "        print('epoch [{}/{}], loss:{:.4f}, completed at {}'\n",
    "            .format(epoch+1, NUM_EPOCHS, loss.data.item(), datetime.now()))\n",
    "        loss_epochs.append(loss)\n",
    "\n",
    "    save_state(\"./model_state.pth\", model, optimizer, loss_epochs)\n",
    "    np.savetxt(\"./convergence_loss.csv\", loss_epochs, delimiter=',')\n",
    "    plt.plot(range(1,len(loss_epochs)+1), loss_epochs)\n",
    "    plt.title(\"Convergence of loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(\"./convergence_plot.png\")\n",
    "\n",
    "def save_state(path, model, optimizer, loss): # epoch, loss\n",
    "    torch.save({\n",
    "            'model': str(model),\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #'epoch': epoch,\n",
    "            'loss': loss,\n",
    "            'w_length': model.w_length\n",
    "            }, path)\n",
    "\n",
    "def load_state(path, model, optimizer, print_model=True): # epoch, loss\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.w_length = checkpoint['w_length']\n",
    "    if(print_model == True):\n",
    "        model_state = checkpoint['model']\n",
    "        print(model_state)\n",
    "    #epoch = checkpoint['epoch']\n",
    "    #loss = checkpoint['loss']\n",
    "\n",
    "def view_state(model, optimizer, state_size = 0):\n",
    "    if(state_size == 1):\n",
    "        print(\"Model -\",model)\n",
    "        return\n",
    "    print(\"Model's state_dict:\")\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor])\n",
    "    print(\"Optimizer's state_dict:\")\n",
    "    for var_name in optimizer.state_dict():\n",
    "        print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "\n",
    "def gram_schmidt(vv):\n",
    "    def projection(u, v):\n",
    "        return (v * u).sum() / (u * u).sum() * u\n",
    "\n",
    "    nk = vv.size(1) # debugged from original repo\n",
    "    uu = torch.zeros_like(vv, device=vv.device)\n",
    "    uu[:, 0] = vv[:, 0].clone() # copy first column\n",
    "    for k in range(1, nk):\n",
    "        vk = vv[:, k].clone() # debugged from original repo\n",
    "        uk = 0\n",
    "        for j in range(0, k): # project vk onto space spanned by bases so far\n",
    "            uj = uu[:, j].clone()\n",
    "            uk = uk + projection(uj, vk)\n",
    "        uu[:, k] = vk - uk\n",
    "    for k in range(nk):\n",
    "        uk = uu[:, k].clone()\n",
    "        uu[:, k] = uk / uk.norm()\n",
    "    return uu\n",
    "# source: https://github.com/legendongary/pytorch-gram-schmidt\n",
    "\n",
    "# helper routine\n",
    "def conv_shape(L, K, S, P):\n",
    "    return (L + 2*P - K) // S + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
